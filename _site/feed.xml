<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RoboComp</title>
    <atom:link href="http://robocomp.github.io/website/feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://robocomp.github.io/website/</link>
    <description>Robotics Framework</description>
    <pubDate>Thu, 20 Aug 2015 10:29:49 +0530</pubDate>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; Symbolic planning techniques for recognizing objects domestic &lt;p&gt;#5&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/08/16/mercedes5.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/08/16/mercedes5.html</guid>
        <description>&lt;p&gt;&lt;strong&gt;Full object manipulation system&lt;/strong&gt; : In this post we will describe the full system developed for manipulating objects using a robotic arm. We start at the highest level, VisualIK (the correction system), and we will go down to the base of the system, the IK (which calculates the final angle of the joints of the robotic arm).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/mercedes92/VisualIKExperiment/blob/master/images/Dibujo%20sin%20t%C3%ADtulo.png?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All the components that will be described below implement the &lt;code&gt;InverseKinematics&lt;/code&gt; interface. This interface is defined by a series of data structures:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;Pose6D&lt;/code&gt;: It represents a pose with three translations and three rotations [x, y, z, rx, ry, rz]&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;WeightVector&lt;/code&gt;: It represents the weight vector of each element of Pose6D. This will help us later in the Levenberg-Marquardt algorithm.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;TargetState&lt;/code&gt;: It is the state when the targets reach the end of their execution by ik. It has some information like the elapsed time, the final error in translations and rotations and the values of each joint.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Axis&lt;/code&gt;: This structure represents the Cartesian axes. It is necessary for certain special types of targets.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Motor&lt;/code&gt;: This structure stores the name and the angular value of a motor of the kinematic chain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also, the interface defines a number of methods:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;getTargetState&lt;/code&gt;: this method returns the state of one target, given the name of the robot part that executed the target and the numerical identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;getPartState&lt;/code&gt;: this method returns the state (if the part has pending targets or not) of one of the robot part (RIGHTARM, LEFTARM or HEAD).&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setTargetPose6D&lt;/code&gt;: This method is used to send targets to the ik. We need to indicate the part of the robot that will execute the target, the pose of the target and the weight vector of the pose. It returns the identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setTargetAlignaxis&lt;/code&gt;: This method sends a special target for the IK. This target is achieved by aligning the end effector to the target axes. We must indicate the the part of the robot that will execute the target, the pose and the axes. It returns the identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setTargetAdvanceAxis&lt;/code&gt;: This method sends another special target for the IK. The goal is that the end effector advance along a vector in the Cartesian system. We must indicate the part of the robot that will execute the target, the axes of the vector and the dist to advance. Like always, it return the identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;goHome&lt;/code&gt;: this method send a robot part to the idle position.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;stop&lt;/code&gt;: this method abort the execution of the targets of one of the robot part.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setJoint&lt;/code&gt;: this method changes the angular value of one of the joint of the kinematic chain.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;setFingers&lt;/code&gt;: this method opens and closes the fingers of the end effector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, we will describe the basic operation of our components.&lt;/p&gt;

&lt;h3 id=&quot;the-visualik-component&quot;&gt;The &lt;code&gt;visualIK&lt;/code&gt; component&lt;/h3&gt;

&lt;p&gt;The old &lt;a href=&quot;http://robocomp.github.io/website/2015/06/17/mercedes3.html&quot;&gt;&lt;code&gt;VisualBIK&lt;/code&gt; component&lt;/a&gt; has been reorganized, resulting in the current &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/visualik&quot;&gt;&lt;code&gt;visualik&lt;/code&gt; component&lt;/a&gt;. The basic principle of operation of the component remains, adding some improvements and changing its communication with the &lt;code&gt;inversekinematics&lt;/code&gt; component, by the &lt;code&gt;ikGraphGenerator&lt;/code&gt; component.&lt;/p&gt;

&lt;p&gt;Its goal remains the same, correct errors produced by the inaccuracies of the joints in the IK. To this end, it bases its operation on a state machine:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;IDLE&lt;/code&gt;: It is the resting state of &lt;code&gt;visualik&lt;/code&gt;. The component is waiting to receive a target through one of the methods of its interface. When a target comes through the interface (stored in the &lt;code&gt;currentTarget&lt;/code&gt; variable), the state machine switches to INIT_BIK and prepares the global variables for the execution of correction.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;INIT_BIK&lt;/code&gt;: in this state, the visualik applies an initial correction to the target (Ec). This correction is based on experience in the correction of previous targets, so the first correction, having no previous experience, will be zero. Then, it sends the target to his proxy of kinematic, the &lt;code&gt;ikGraphGenerator&lt;/code&gt; component. Finally, the state of the machine is changed to &lt;code&gt;WAIT_BIK&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;WAIT_BIK&lt;/code&gt;: in this state, the &lt;code&gt;visualik&lt;/code&gt; waits the end of execution of the target by the &lt;code&gt;ikGraphGenerator&lt;/code&gt;. When the &lt;code&gt;ikGraphGenerator&lt;/code&gt; finishes executing the target, the &lt;code&gt;visualik&lt;/code&gt; changes his state to &lt;code&gt;CORRECT_ROTATION&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;CORRECT_ROTATION&lt;/code&gt;: It is the latest machine status. In this state is when the &lt;code&gt;visualik&lt;/code&gt; does all the calculations in order to correct the deviations and errors of the joints. The procedure is simple: a) by apriltags, the &lt;code&gt;visualik&lt;/code&gt; calculates the visual pose of the end effector; b) then, it computes the error vector between the visual pose and the target pose (Ev); c) with this error vector Ev, the &lt;code&gt;visualik&lt;/code&gt; corrects the target pose and sends the new position to the &lt;code&gt;ikGraphGenerator&lt;/code&gt; component; d) this process is repeated until the error achieved in translation and rotation is less than a predetermined threshold; e) finally the &lt;code&gt;visualik&lt;/code&gt; calculates the error vector between the original target and the last corrected target (Ec), with this error the component realizes the first correction in &lt;code&gt;INIT_BIK&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here there is a scheme of the procedure performed by the &lt;code&gt;visualik&lt;/code&gt;. It is very summarized to facilitate the understanding of the procedure by the reader:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1: procedure VISUAL CALIBRATION
2:    Xt = TargetArrives()
3:    Ct = Xt + Ec
4:    sendTargetToGIK(Ct)
5:    Xv = getAprilTagPose(robot)
6:    while (Ev = Xv- Xt) &amp;gt; threshold ^¬ timeOut() do
7:         Xi = getInternalPose(robot)
8:         Xc = Xi + Ev
9:         sendTargetToGIK(Xc)
10:        Xv = getAprilTagPose(robot)
11:   end while
12:   Ec = Xc - Xt
13: end procedure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Like all components developed by robocomp, the &lt;code&gt;visualik&lt;/code&gt; needs a configuration file in which the components required (the &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator&quot;&gt;&lt;code&gt;ikGraphGenerator&lt;/code&gt;&lt;/a&gt;) and the components to subscribe (the &lt;a href=&quot;https://github.com/robocomp/robocomp-robolab/tree/master/components/apriltagsComp&quot;&gt;&lt;code&gt;apriltagsComp&lt;/code&gt;&lt;/a&gt;) and other configuration parameters are determined. In this case, a configuration file may have the following elements:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CommonBehavior.Endpoints=tcp -p 14537
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10242

# Endpoints for interfaces to subscribe:
AprilTagsTopic.Endpoints=tcp -p 12938

# Proxies for required interfaces
InverseKinematicsProxy =inversekinematics:tcp -h localhost -p 10241 # `ikGraphGenerator`

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus-rockin/etc/ficheros_Test_VisualBIK/ursus_bik.xml # the internal model of the robot environment 

# This property is used by the clients to connect to IceStorm.
TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;the-ikgraphgenerator-component&quot;&gt;The &lt;code&gt;ikGraphGenerator&lt;/code&gt; component&lt;/h3&gt;

&lt;p&gt;As we explain in the previous &lt;a href=&quot;http://robocomp.github.io/website/2015/08/13/mercedes4.html&quot;&gt;post&lt;/a&gt;, the &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator&quot;&gt;&lt;code&gt;ikGraphGenerator&lt;/code&gt;component&lt;/a&gt; creates and stores a graph representing the work 3D space of the arm, where each node stores the euclidean space pose of the end effector and the configuration of the joints that compose the arm. So the &lt;code&gt;ikGraphGenerator&lt;/code&gt; waits until the &lt;code&gt;visualik&lt;/code&gt; send a target to him through the &lt;code&gt;InverseKinematics&lt;/code&gt; interface. In this moment, the &lt;code&gt;ikGraphGenerator&lt;/code&gt; performs four steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the component searches the node A whose pose is closest to the initial end effector pose and moves the arm there.&lt;/li&gt;
  &lt;li&gt;the component finds in the graph  the node B whose pose is closest to the target position, disabling those nodes which would make the robot’s arm collide with external objects.&lt;/li&gt;
  &lt;li&gt;the component computes the shortest path between the node A and the node B and moves the end effector among the nodes to reach the node B.&lt;/li&gt;
  &lt;li&gt;Finally, the component moves from the graph to the actual target sending the original target to the &lt;code&gt;inversekinematic&lt;/code&gt;component and taking the final values of the joints.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;code&gt;ikGraphGenerator&lt;/code&gt;needs a config file too:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CommonBehavior.Endpoints=tcp -p 14536
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10241

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml # the internal model of the robot environment 

# Proxies for required interfaces
InverseKinematicsProxy = inversekinematics:tcp -h localhost -p 10240
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

Ice.Warn.Connections=0
Ice.Trace.Network=0 
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;the-inversekinematic-component&quot;&gt;The &lt;code&gt;inversekinematic&lt;/code&gt; component&lt;/h3&gt;

&lt;p&gt;Finally, we will explain the basic component of the system, the &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/inversekinematics&quot;&gt;&lt;code&gt;inversekinematic&lt;/code&gt; component&lt;/a&gt;. As we said in the &lt;a href=&quot;http://robocomp.github.io/website/2015/06/15/mercedes2.html&quot;&gt;second post&lt;/a&gt;, this component receives three types of targets through the &lt;code&gt;InverseKinematics&lt;/code&gt; interface:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;POSE6D&lt;/code&gt;: the typical target with translations and rotations [tx, ty, tz, rx,ry, rz]. The end effector has to be positioned at coordinates (tx, ty, tz) of the target and align their rotation axes with the target, specified in (rx, ry, rz). This target arrives from the method &lt;code&gt;setTargetPose6D&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ADVANCEAXIS&lt;/code&gt;: its goal is to move the end effector of the robot along a vector. This target arrives from the method &lt;code&gt;setTargetAdvanceAxis&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ALIGNAXIS&lt;/code&gt;: Its goal is that the end effector has the axes aligned with the target. This target arrives from the method &lt;code&gt;setTargetAlignaxis&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Targets received are stored into the queues of the corresponding robot part and they are executed by order of arrival. When the ik ends the execution of one target, it stores the target into the solved targets queue:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/mercedes92/VisualIKExperiment/blob/master/images/iksystem.png?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The config file of the &lt;code&gt;inversekinematic&lt;/code&gt;component is the next:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CommonBehavior.Endpoints=tcp -p 12207
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10240

# Kinematic chain lists: they stores the joints names.
RIGHTARM=rightShoulder1;rightShoulder2;rightShoulder3;rightElbow;rightForeArm;rightWrist1;rightWrist2
RIGHTTIP=grabPositionHandR # end effector of the RIGHTARM

LEFTARM=leftShoulder1;leftShoulder2;leftShoulder3;leftElbow;leftForeArm;leftWrist1;leftWrist2
LEFTTIP=grabPositionHandL

HEAD=head_yaw_joint;head_pitch_joint
HEADTIP=rgbd_transform

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml #Internal model of the robot environment

 # Proxies for required interfaces
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When &lt;code&gt;visualik&lt;/code&gt; and &lt;code&gt;ikGraphGenerator&lt;/code&gt; components submit their targets to the immediately below component, they store the identifier of the target and are waiting until the target be resolved, calling the method &lt;code&gt;getTargetState&lt;/code&gt;. So, when the &lt;code&gt;inversekinematic&lt;/code&gt; component ends one target execution, the &lt;code&gt;ikGraphGenerator&lt;/code&gt; moves the arm with the values given by the &lt;code&gt;inversekinematic&lt;/code&gt; component and then, the &lt;code&gt;visualik&lt;/code&gt; continues with the corrections.&lt;/p&gt;

&lt;p&gt;Having explained the handling system, the next post will explain the planning system developed by ROBOLAB to plan the robot’s actions.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;

</description>
        <pubDate>Sun, 16 Aug 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; Symbolic planning techniques for recognizing objects domestic &lt;p&gt;#4&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/08/13/mercedes4.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/08/13/mercedes4.html</guid>
        <description>&lt;p&gt;&lt;strong&gt;ikGraphGenerator, an alternative to ik&lt;/strong&gt; : As the project has progressed, many improvements have emerged. One of them is the new component, &lt;code&gt;ikGraphGenerator&lt;/code&gt;. This component has been developed by Professor Luis Manso, and its goal is to remove weight to the inverse kinematics in the process of handling objects with a robotic effector.&lt;/p&gt;

&lt;p&gt;One of the problems of the inverse kinematics is that, given a target for a particular end-effector, it can calculate different solutions or values for each motor of the kinematic chain. For example, to pick up a cup, the IK can position the end effector at the target extending his arm more or less forcing or not the elbow or or separating more or less the shoulder. It doesn’t matter that the final position of the chain be more forced or more natural, the goal is reached and the solution is accepted.&lt;/p&gt;

&lt;p&gt;This is not convenient, since there is no way to control the trajectory of the arm. The control on the trajectory arm is crucial at certain times, for example when the robot avoids collisions between his arm and another objects (tables, chairs, walls, including his own body). The current &lt;code&gt;inversekinematics&lt;/code&gt; component does not allow us this control, so that we need the &lt;code&gt;ikGraphGenerator&lt;/code&gt; component.&lt;/p&gt;

&lt;p&gt;The main concept is to create a spatial graph where each node stores the pose of the point, [tx, ty, tz, rx, ry, rz], and the set of angular values for each motor of the kinematic chain (that has 7 DOFs). The links connecting the nodes whose positions are close to each other. In this way we can calculate paths between two different and separate points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/ikg.jpg?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-does-the-ikgraphgenerator-component&quot;&gt;What does the &lt;code&gt;ikGraphGenerator&lt;/code&gt; component?&lt;/h3&gt;

&lt;p&gt;Basically the &lt;code&gt;ikGraphGenerator&lt;/code&gt; realizes two functions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It calculates the graph with random poses and their respective angular values. To do this, first it defines a spatial cube that represents the workspace of the robot arm. In this space a set of poses are selected and are sent to the &lt;code&gt;inversekinematic&lt;/code&gt; component as targets. The poses that are not achievable by the ik are automatically deleted. The resultant graph is stored in a file in order to use it in later calculations.&lt;/li&gt;
  &lt;li&gt;Once the graph has been calculated and stored, we can send a target to the &lt;code&gt;ikGraphGenerator&lt;/code&gt;. First the component searches the node &lt;code&gt;A&lt;/code&gt; whose pose is closest to the initial end effector pose and the node &lt;code&gt;B&lt;/code&gt; whose pose is closest to the target position (to do this, the &lt;code&gt;ikGraphGenerator&lt;/code&gt; uses a fast low-dimension k-d tree). Then, the component calculates a path between node A and node B through the graph  using Dijkstra’s algorithm, so that the arm moves through the graph from the position marked by node A to the position marked by the node B. Finally, in order to achieve the final target, the &lt;code&gt;inversekinematic&lt;/code&gt; component is called to compute the final values of the joints, starting from the position marked by the node B.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/GIK.png?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the next post I will describe how the whole system works with all the components, the &lt;code&gt;inversekinematic&lt;/code&gt;, the &lt;code&gt;ikGraphGenerator&lt;/code&gt; and the &lt;code&gt;visualik&lt;/code&gt; component.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Aug 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>Till now ... after midterm</title>
        <link>http://robocomp.github.io/website/2015/08/08/nithin9.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/08/08/nithin9.html</guid>
        <description>&lt;p&gt;Hi all , In this post i will talk about what i have been working on after midterm evaluation. I have spend my time working mostly on packaging supporting libraries for Robocomp.This includes FCL and libccd. FCL is a library for performing three types of proximity queries on a pair of geometric models composed of triangles. libccd is a library for collision detection between two convex shapes.Technically Robocomp is only using FCL but libccd is an dependency of fcl, as i couldn’t find an updated ppa for it i decided to package it too. you can see those packages &lt;a href=&quot;https://launchpad.net/~imnmfotmal&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also i added ability for generating robocomp source packages for different distributions. Initially i added the option only for trusty, now we could generate packages for any distribution.I worked a bit on build tools also. Currently if someone created an workspace and made a github repo of it, some one else cant use it as we store the repo names in ~/.config directory, so i added an option to reinit an repo.&lt;/p&gt;

&lt;p&gt;well i guess thats all for now&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</description>
        <pubDate>Sat, 08 Aug 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>Setting up ppa</title>
        <link>http://robocomp.github.io/website/2015/07/25/nithin10.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/07/25/nithin10.html</guid>
        <description>&lt;h2 id=&quot;setting-up-an-ppa-in-launchpad&quot;&gt;Setting up an ppa in launchpad&lt;/h2&gt;

&lt;p&gt;After creating an launchpad account First you need to create and publish an OPENPGP key&lt;/p&gt;

&lt;h3 id=&quot;generating-your-key-in-ubuntu&quot;&gt;Generating your key in Ubuntu&lt;/h3&gt;
&lt;p&gt;The easiest way to generate a new OpenPGP key in Ubuntu is to use the Passwords and Encryption Keys tool.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; open Passwords and Encryption Keys.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Select File &amp;gt; New, select PGP Key and then follow the on-screen instructions.&lt;/p&gt;

&lt;p&gt;Now you’ll see your new key listed in the Passwords and Encryption Keys tool. (it may take some time)&lt;/p&gt;

&lt;h3 id=&quot;publishing-your-key&quot;&gt;Publishing your key&lt;/h3&gt;

&lt;p&gt;Your key is useful only if other people can verify items that you sign. By publishing your key to a keyserver, which acts as a directory of people’s public keys, you can make your public key available to anyone else.Before you add your key to Launchpad, you need to push it to the Ubuntu keyserver.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Open Passwords and Encryption Keys.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Select the My Personal Keys tab, select your key.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;  Select Remote &amp;gt; Sync and Publish Keys from the menu. Choose the Sync button. (You may need to add htp://keyserver.ubuntu.com to your key servers if you are not using Ubuntu.)&lt;/p&gt;

&lt;p&gt;It can take up to thirty minutes before your key is available to Launchpad. After that time, you’re ready to import your new key into Launchpad!&lt;/p&gt;

&lt;p&gt;OR you can direclty to go &lt;code&gt;http://keyserver.ubuntu.com/&lt;/code&gt; on your browser and add the PGP key there&lt;/p&gt;

&lt;h3 id=&quot;register-your-key-in-launchpad&quot;&gt;Register your key in launchpad&lt;/h3&gt;
&lt;p&gt;fire up an terminal and run &lt;code&gt;gpg --fingerprint&lt;/code&gt; should give you fingerprints of all the keys. copy paste the required fingerprint into launchpad&lt;/p&gt;

&lt;h3 id=&quot;sign-ubunutu-code-of-conduct&quot;&gt;Sign Ubunutu Code of Conduct&lt;/h3&gt;
&lt;p&gt;Download the ubuntu code of conduct form launchpad
&lt;code&gt;gpg --clearsign UbuntuCodeofConductFile&lt;/code&gt;  will sign the file
now copy the contents of the signed file and paste in launchpad&lt;/p&gt;

&lt;h3 id=&quot;wrapping-up&quot;&gt;Wrapping Up&lt;/h3&gt;
&lt;p&gt;Now everything is set up. make sure you have some key in &lt;code&gt;OPENPGP Keys&lt;/code&gt; section and also the signed code of code of conduct as &lt;code&gt;Yes&lt;/code&gt; as shown.
&lt;img src=&quot;./launchpad.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;uploading-package-to-ppa&quot;&gt;Uploading package to ppa&lt;/h2&gt;

&lt;p&gt;launchpad will only accept source packages and not binary.Launchpad will then build the packages. For building source packages we are using debuild which is a wrapper around the &lt;em&gt;dpkg-buildpackage + lintian&lt;/em&gt;. so you will need to install debuild and dput on your system;&lt;/p&gt;

&lt;p&gt;The source_package.cmake script is used to create debian source package.&lt;/p&gt;

&lt;p&gt;The main CMakeLists.txt file defines a target &lt;code&gt;spackage&lt;/code&gt; that builds the source package in build/Debian with &lt;code&gt;make spackage&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For uploading the package to ppa, First change the &lt;strong&gt;PPA_PGP_KEY&lt;/strong&gt; in &lt;a href=&quot;../cmake/package_details.cmake#L26&quot;&gt;package_details.cmake&lt;/a&gt; to details to the full-name of the PGP key  details registered with your ppa account For more details on setting up the pgp key see the &lt;a href=&quot;./setting_up_ppa.md&quot;&gt;tutorial&lt;/a&gt;.Then create a source package by building the target &lt;em&gt;spackage&lt;/em&gt;.Once the Source package is build successfully, upload it to your ppa by:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd Debian/
dput ppa:&amp;lt;lp-username&amp;gt;/&amp;lt;ppa-name&amp;gt; &amp;lt;packet-&amp;gt;source.changes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;building of source package can be tested with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd Debian/robocomp-&amp;lt;version&amp;gt;
debuild -i -us -uc -S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you are uploading a new version of robocomp, change the version number  accordingly in the &lt;a href=&quot;../CMakeLists.txt#L31&quot;&gt;toplevel cmake&lt;/a&gt; before building, and then upload the source package as mentioned.&lt;/p&gt;

&lt;h3 id=&quot;note&quot;&gt;Note:&lt;/h3&gt;

&lt;p&gt;If you want to upload another source package to ppa which doesn’t have any changes in the source but maybe in the debian files. you can build the spackage after commenting out &lt;code&gt;set(DEB_SOURCE_CHANGES &quot;CHANGED&quot; CACHE STRING &quot;source changed since last upload&quot;)&lt;/code&gt; in &lt;a href=&quot;../cmake/package_details.cmake#L27&quot;&gt;package_details.cmake&lt;/a&gt; so that the the script will only increase the ppa version number and won’t include the source package for uploading to ppa (which otherwise will give an error).&lt;/p&gt;

&lt;h2 id=&quot;installing-robocomp-from-ppa&quot;&gt;Installing robocomp from ppa&lt;/h2&gt;

&lt;p&gt;First you will need to add the ppa in your sources, and then install robocomp package.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo add-apt-repository ppa:&amp;lt;lp-username&amp;gt;/robocomp
sudo apt-get update
sudo apt-get install robocomp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;this will install robocomp along with basic components into /opt/robocomp.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</description>
        <pubDate>Sat, 25 Jul 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; Computer vision components and libraries management &lt;p&gt;#1&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/07/02/kripa1.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/07/02/kripa1.html</guid>
        <description>&lt;p&gt;&lt;strong&gt;About me&lt;/strong&gt;:Hello, I am Kripasindhu Sarkar, a new PhD student at German Research Center for Artificial Intelligence (DFKI), Kaiserslautern working in the topic of Object Detection in simple and depth images. 
I am extremely interested in the topic of object detection and computer vision; specifically in solving the problem by using theories from human cognition and perception to simulate human way of visualizing the problem. 
But for now, I am focused on getting a very good grasp at the existing engineering (mostly) techniques in the field of computer vision and object detection. 
Before joining here as a PhD student I worked as a Software Engineer at Paypal for 2 years and, prior to that I did my masters and graduation from Indian Institute of Technology Kharagpur (IIT Kharagpur).&lt;/p&gt;

&lt;h2 id=&quot;computer-vision-components-and-libraries-management&quot;&gt;Computer vision components and libraries management&lt;/h2&gt;

&lt;p&gt;The project is about designing and implementing a system for object detection and recognition in 3D point clouds and 2D images, and come up with a structured library with a good and easy-to-use APIs.
There has been a good amount of research in this direction and my work was to cherry-pick important ideas and present them as usable components. I’ll now explain in details the various methods I chose to
use as a part of this project.&lt;/p&gt;

&lt;h3 id=&quot;local-feature-based-on-2d-images&quot;&gt;Local feature based on 2D images&lt;/h3&gt;
&lt;p&gt;The idea is the find local features (like SIFT/SURF/ORB etc) in images of the object to be detected and the given test image. If enough matches are found between the descriptors of the to images an object is defined to be found. Important assumption is that the object to be detected must have textures. Advantage is that we get the complete 6 DOF of the object which might be useful for grasping. This comes in several flavors.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Planner objects: If we know the object is planner, we can directly compute its tomography (pose) after the match.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Random objects: If the object is of arbitrary shape it is quite difficult to detect an object with its pose but can be done in a tricky offline phase [1]. A 3D reconstruction is performed through bundle adjustments with the object to be detected to find the 2D - 3D correspondences. On the run time, given an input image, If enough matches are found, the object is detected with its full pose by solving PnP problem.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dense-feature-based-on-2d-images&quot;&gt;Dense feature based on 2D images:&lt;/h3&gt;
&lt;p&gt;The idea is the find features over a grid or a region of an image encoding the properties of that region and use that feature in some classification algorithm to perform detection. Naturally, we need to calculate dense feature over all possible region size over the image and apply the classifier; and thus it is bit slow as well. Also object pose is not identified in this type. Few of them are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;HOG based simple classification (well known).
Difficulty in implementation: Moderate; HOG implementation with multiscale detector is present in OpenCV; but the training has to be performed separately using 3rd party tool like libsvm/matlab etc (but is straightfwd).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HOG based Part Based Model: This is the famous and legendary and state of art (not anymore) object detector which uses LSVM.
Difficulty in implementation: Difficult; OpenCV has the detection code, but not that good. Training LSVM is not straight fwd and we need to use the original Matlab implementation of the authors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Wevlet based face detector with adaboost: This is also well known face detector algorithm used widely.
Difficulty in implementation: Easy; though the concept is not that straight fwd, it is readily avilable in OpenCV.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;detectionrecognition-on-depth-images&quot;&gt;Detection/Recognition on Depth Images&lt;/h3&gt;
&lt;p&gt;If we can get the Point Cloud with some laser scan or Kinect, there are plenty of algorithms to detect object with its pose. Again we have local feature based and global feature based algorithms described below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Direct object with local and global features [4]:
Very similar to that of RGB image based algorithm with difference in the types of features. Local features have the advantage that preprocessing steps like segmentation is not required but tends to be slow. On the other hand we need to do segmentation to apply global features in the clusters. But once the segmentation (like identifying planes, etc and different clusters) of the scene is done, we can use the results subsequently. 
Difficulty in implementation: Easy; components of pipeline is available in PCL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Object matching using classifiers: 
Global features readily available in PCL and found it to have similar results to a current benchmark but faster (10 seconds for classification testing in the benchmark [2] which uses sliding window based classification on all scales using HOG like descriptors).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-library---open-detection&quot;&gt;The library - Open Detection&lt;/h2&gt;
&lt;p&gt;It was decided later to have an independent library for Object Detection instead of integrating everything to Robocomp. The result is the inception of a separate library ‘Open Detection’.
The details of the design of the library is discussed in the next blog.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;[1] I. Gordon and D. G. Lowe, “What and where: 3d object recognition with accurate pose,” in Toward Category-Level Object Recognition, ser. Lecture Notes in Computer Science, J. Ponce, M. Hebert, C. Schmid, and A. Zisserman, Eds., vol. 4170. Springer, 2006, pp. 67–82.
[2] MOPED: A Scalable and Low Latency Object Recognition and Pose Estimation System
[3] Object Detection with Discriminatively Trained Part Based Models
[4] Aldoma, A.; Marton, Zoltan-Csaba; Tombari, F.; Wohlkinger, W.; Potthast, C.; Zeisl, B.; Rusu, R.B.; Gedikli, S.; Vincze, M., “Tutorial: Point Cloud Library: Three-Dimensional Object Recognition and 6 DOF Pose Estimation,” Robotics &amp;amp; Automation Magazine, IEEE , vol.19, no.3,&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jul 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; Computer vision components and libraries management &lt;p&gt;#2&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/07/02/Kripa2.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/07/02/Kripa2.html</guid>
        <description>&lt;p&gt;&lt;strong&gt;Open Detection:&lt;/strong&gt; Following the idea that it is better to have an independent library for Object Detection than contributing directly to Robocomp, I created the new library ‘Open Detection’. It is available now in the following links&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Github link: https://github.com/krips89/opendetection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Documentation link: http://krips89.github.io/opendetection_docs/index.html&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have tried to document/provide tutorial inside the library whenever possible. So instead of writing everything here in the blog I’ll just post links to the tutorials/documentations.&lt;/p&gt;

&lt;h3 id=&quot;installation-instructions&quot;&gt;Installation Instructions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Link: https://github.com/krips89/opendetection/blob/master/doc/tutorials/content/installation_instruction.rst&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;library-design&quot;&gt;Library Design&lt;/h3&gt;
&lt;p&gt;The basic idea was to have a library with common and simple interface giving access to varies detection methods available here. After some thinking I came up with the design explained in the following tutorial:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://github.com/krips89/opendetection/blob/master/doc/tutorials/content/basic_structures.rst&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The class diagrams providing a good reference is provided here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://krips89.github.io/opendetection_docs/inherits.html&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;documentation&quot;&gt;Documentation&lt;/h3&gt;
&lt;p&gt;I did not document extensively till now as building an independent library from the scratch took a long time. The other very important reason is that the design is till little vulnerable to changes.
I would wait little bit more for the design to be more concrete before I start documenting extensively.&lt;/p&gt;

&lt;h2 id=&quot;things-finished&quot;&gt;Things finished&lt;/h2&gt;
&lt;p&gt;Within this time frame I could finish the following tasks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Design of the library.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Complete CMake infrastructure for modular building of the library from scratch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2D feature based object detection (both Training and Detection phase) with demo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Global feature based object detection (both training and detection phase) with demo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Auto generated Documentation using Doxygen (http://krips89.github.io/opendetection_docs/index.html).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sphinx based tutorial section to generate nice pages for tutorials and blogs like that of PCL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Few other Utility classes which fits the needs and design for the library.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;milestones-and-things-learnt&quot;&gt;Milestones and things learnt:&lt;/h3&gt;
&lt;p&gt;In the next blog I’ll add the different sources I used to design and implement the above tasks and the things I learnt in this process.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Kripasandhu Sarkar&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jul 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>Build tools</title>
        <link>http://robocomp.github.io/website/2015/06/26/nithin6.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/26/nithin6.html</guid>
        <description>&lt;h3 id=&quot;rcinitws&quot;&gt;rc_init_ws&lt;/h3&gt;
&lt;p&gt;This will initialize a Robocomp workspace in the current/specified directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rc_init_ws [path]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;rcbuild&quot;&gt;rcbuild&lt;/h3&gt;
&lt;p&gt;When invoked form workspace without any arguments if not inside source path, it will build all the non-ignored components inside the workspace,  if inside any component source directory it will build only that component. But if a component is specified it will build it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; rcbuild [-h] [-i | --doc | --installdoc] [component]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;doc&lt;/code&gt; will generate documentation, &lt;code&gt;installdoc&lt;/code&gt; will install the docs to install path, &lt;code&gt;install&lt;/code&gt; will build and install the components. currently you can only generate docs for one component at a time.&lt;/p&gt;

&lt;h3 id=&quot;rccomp&quot;&gt;rccomp&lt;/h3&gt;
&lt;p&gt;This dosent have much functions as of now. &lt;code&gt;rccomp list&lt;/code&gt; will list all the components.&lt;/p&gt;

&lt;h3 id=&quot;rced&quot;&gt;rced&lt;/h3&gt;
&lt;p&gt;when invoked as component-name file-name. it will open the the file in the component. if multiple files with same name exists, it will give choices and will ask you to choose one. It uses the editor specified in $EDITOR by default, if not present it will use &lt;code&gt;vim&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rced [-h] component file
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;rcrun&quot;&gt;rcrun&lt;/h3&gt;
&lt;p&gt;Using rcrun you can start, stop or force stop any component from anywhere. You can also start a component in debug mode, given you have the required &lt;em&gt;config file&lt;/em&gt; in the &lt;em&gt;etc&lt;/em&gt; directory. If you have specified a config file then rcrun will use it to start the component. By default rcrun will use the &lt;code&gt;config&lt;/code&gt; config file in &lt;code&gt;etc&lt;/code&gt; directory, if not found it will search for &lt;code&gt;generic_config&lt;/code&gt; if not found it will use any of config files present.If the debug flag is set, it will search for a config file that ends with &lt;em&gt;.debug&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rcrun [-h] [-s START |-st STOP | -fst FSTOP] [-d | -cf CFILE | -c CONFIG] [-is] [component]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;rccd&quot;&gt;rccd&lt;/h3&gt;
&lt;p&gt;Using this you can cd into the component directory given the component name.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rccd component
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; Till Now ... &lt;p&gt;#1&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/06/25/rajath1.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/25/rajath1.html</guid>
        <description>&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Built a website for robocomp using jekyll.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Progress:&lt;/strong&gt; Took the task of building a website for documenting the open source project &lt;em&gt;RoboComp&lt;/em&gt; for the first 4 weeks of Google Summer of Code 2015. The website should be able to segregate the posts into categories, Make it easy for users to post content and most importantly have a proper flow among the posts so that a new users will find it easy to learn the framework.&lt;/p&gt;

&lt;p&gt;Started building the website by using the &lt;a href=&quot;https://github.com/dbtek/dbyll&quot;&gt;dbyll theme&lt;/a&gt;. Messed around with the code a bit and had the website up for robocomp. Website &lt;a href=&quot;https://rajathkumar.github.io/robocomp&quot;&gt;link&lt;/a&gt;. After a few iterations the website was all good. While exploring on the same topic stumbled upon &lt;a href=&quot;https://rohanchandra.github.io/project/type/&quot;&gt;Type theme&lt;/a&gt;. Which was a jekyll based them more clean and elegant than the first one. Ported the entire website to the type theme and currently have shifted the &lt;a href=&quot;http://robocomp.github.io/website/&quot;&gt;website&lt;/a&gt; to the &lt;a href=&quot;https://github.com/robocomp&quot;&gt;robocomp organization on github&lt;/a&gt;. Currently have tweaked the website based on the suggetsions recieved by the mentors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Future:&lt;/strong&gt; Will implement automatic segregation of posts based on categories the user mention. Extend the features of the website and add analytics, comments for blog posts etc. Give the website its final iterations based on the suggestions recieved.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Rajath Kumar M.P&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>New build system and workspace model in Robocomp &lt;p&gt;#2&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/06/25/nithin5.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/25/nithin5.html</guid>
        <description>&lt;p&gt;For managing these components we would need different utilities. So i started compiling a list of different utilities keeping a reference to other frameworks. Finally i have decide on my list&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rc_init_ws&lt;/li&gt;
  &lt;li&gt;rcbuild&lt;/li&gt;
  &lt;li&gt;rced&lt;/li&gt;
  &lt;li&gt;rccd&lt;/li&gt;
  &lt;li&gt;rcrun&lt;/li&gt;
  &lt;li&gt;rccomp&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information about the utilities see the &lt;a href=&quot;http://robocomp.github.io/website/2015/06/26/nithin6.html&quot;&gt;tutorial&lt;/a&gt; on build utilities. All the utilities are implemented using python except &lt;code&gt;rccd&lt;/code&gt; which is implemented as a shell function, As a subprocess cant affect its parents environment. So i was thinking, as anyway we will need to source a bash script, we could move the exporting of the environment variables also into that script.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;One useful feature that needs to be implemented is auto complete for the arguments. It would be a very useful feature as we don’t need need to know the exact component name, etc. Also some serious work on the manifest.xml has to be done. It was planned to contain basic info on packages like name, maintainer, dependencies etc. I didn’t do it right now because i was not really sure about the component dependencies, i will need to discuss about it a bit.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; 2015 ideas</title>
        <link>http://robocomp.github.io/website/2015/06/22/gsoc15.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/22/gsoc15.html</guid>
        <description>&lt;p&gt;1.- &lt;strong&gt;RoboComp tutorial, social management and documentation&lt;/strong&gt;: RoboComp’ sources has been ported to GitHub and we are building a new documentation repository there. We are using GitHub markdown language (GFM) write new docs and turorials. We want to build a set of short tutorials that guide the new users along several interconnected topics, such as component oriented programming, robotics, computer vision, robotics software modules integrating heterogeneous sources, cognitive architectures and testing and validating, all from inside RoboComp. These new tutorials will be developed using RoboComp’s robotics simulator, RCIS, so interactive examples can be created and used in the explanations. This package also includes work on automated installation scripts using CMake. Generic knowledge of linux systems, website and wiki administration is needed. This is a key task for our project as it would bring more attention to it as it will open the development to new people interested in the field.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate programming and systems administration.&lt;/p&gt;

&lt;p&gt;2.- &lt;strong&gt;Computer vision components and libraries management&lt;/strong&gt;: RoboComp is being used to build a new cognitive architecture called RoboCog. Among the different modules already in progress, the object detection module is crucially important. We are pursuing an efficient 2D/3D vision pipeline that, working with the robot body control module, is able to localize, recognize, fit a pre-existing model and track a series of daily objects that the robot might encounter. Grasping would be one target of this pipeline, or even a means to complete recognition. There are currently many components implementing computer vision algorithms and intensive work done in pipeline construction. The key tasks on this idea would be to collaborate in the creation of high level tools to organize, document and test different pipelines for an specific task. These tools will be designed in collaboration with the mentors and tested in RoboComp’s RCIS simulator and real robots.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate computer vision knowledge, C++ programming, basic CMake knowledge&lt;/p&gt;

&lt;p&gt;3.- &lt;strong&gt;RoboComp Building and deployment system design&lt;/strong&gt;: Current CMake building system in RoboComp is limited only to the core libraries, the RCIS simulator and some additional tools. A very useful task would be to come up with a more complex CMake structure that could build the entire system, including all finished components, without breaking current dependencies. Also, a few scripts will have to be built to compile individual components, run tests, search and inspect the source tree efficiently, check dependencies and documentation requirements. This code needs also to take into account the dependencies between components that can be stored in xml-like files -i,e, manifestos- within the component itself.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate CMake knowledge, programming in C++, basic knowledge of shell scripting&lt;/p&gt;

&lt;p&gt;4.- &lt;strong&gt;Deployment generator and run-time monitoring&lt;/strong&gt;: When creating a specific robot architecture, many components have to be brought into a common deployment environment. Each component has its runtime configuration and network parameters that have to be declared in a common deployment file, from where the complete system can be brought to life. This task proposes the design of a domain specific language to facilitate the creation of shellscript deployment files that are syntactically and semantically correct. Once a net of RoboComp components is up and running, additional tools are needed to monitorize their execution through an existing default interface called CommonBehavior.This tool will use the DSL as input and will show a graphical representation of the running system. It will be written in Python and will extend important efforts already made in this direction.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate programming with Python and introductory knowledge of formal languages&lt;/p&gt;

&lt;p&gt;For any questions, proposals, or comments please contact RoboComp’s org admin at:
marcogunex.es&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>New build system and workspace model in Robocomp &lt;p&gt;#1&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/06/20/nithin4.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/20/nithin4.html</guid>
        <description>&lt;p&gt;I have started the workspace model design keeping in mind the following points.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;you should be able to build all packages at once, if necessary and also separately&lt;/li&gt;
  &lt;li&gt;the source tree should be kept clean&lt;/li&gt;
  &lt;li&gt;It should scalable and also existing components should be easily moved in&lt;/li&gt;
  &lt;li&gt;dependencies should be easily handled&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Referring to other similar workspace models i came up with the following model.&lt;/p&gt;

&lt;p&gt;The recommended layout for development is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/website/img/workspace_model.jpg&quot; alt=&quot; Robocomp workspace model&quot; title=&quot;Robocomp workspace model&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;elements-of-workspace&quot;&gt;Elements of workspace&lt;/h2&gt;

&lt;h3 id=&quot;workspace&quot;&gt;Workspace&lt;/h3&gt;
&lt;p&gt;The workspace is the folder inside which you are going to be actively developing. Keeping things in a folder with connected development helps keep separation of development models.&lt;/p&gt;

&lt;h3 id=&quot;source-space&quot;&gt;Source space&lt;/h3&gt;
&lt;p&gt;The source space is the folder is where it will be expected to look for packages when building. This folder is easily identified as it is where the toplevel.cmake is linked from the catkin project. Each component should be in a direct subdirectory. if the directory contains a file named &lt;em&gt;IGNORE_COMP&lt;/em&gt; the component will be ignored while building the workspace.&lt;/p&gt;

&lt;h3 id=&quot;build-space&quot;&gt;Build Space&lt;/h3&gt;
&lt;p&gt;The build space is the folder in which cmake is invoked and generates artifacts such as the CMakeCache. A typical invocation of cmake will look like this when following the recommended layout.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cmake ../src
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This need not be a direct sub directory of workspace. It can be any where.&lt;/p&gt;

&lt;h3 id=&quot;development-space&quot;&gt;Development Space&lt;/h3&gt;
&lt;p&gt;The development space is where build system generates the binaries and config files which are executable before installation. This should be a direst subdirectory of workspace. Currently the &lt;code&gt;devel space is merged with the source space&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;install-space&quot;&gt;Install Space&lt;/h3&gt;
&lt;p&gt;If make install is called this is the directory into which cmake will target all installations. This directory contains a file names &lt;em&gt;.rc_install&lt;/em&gt; which contain a semi colon separated paths of workspaces which are installed to this install space. Please note that the robocomp istall path &lt;em&gt;/opt/robocomp&lt;/em&gt; can is also an install space.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; Symbolic planning techniques for recognizing objects domestic &lt;p&gt;#3&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/06/17/mercedes3.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/17/mercedes3.html</guid>
        <description>&lt;p&gt;&lt;strong&gt;Visual inverse kinematics, Basic understanding :&lt;/strong&gt; In the previous post we anticipate the problems caused by the gaps and inaccuracies of motors in the inverse kinematics of the robot. Now, in this third post we will talk about the solution implemented during the GSoC15 project.&lt;/p&gt;

&lt;p&gt;So, with the inverse kinematics component that we have implemented in Robocomp, we had the problem of inaccuracies and gaps in the robotic arm motors, problems that made the robot believed reach the target position without having actually achieved it. To solve this problem it was decided to implement a solution inside the visual field (which is what concerns us throughout this project), whose aim is to provide the inverse kinematics component a visual feedback that allows correct its mistakes. The operation of the algorithm is very simple and takes as its starting point the investigations of Seth Hutchinson, Greg Hager and Peter Corke, collected in &lt;code&gt;A Tutorial on Visual Servo Control&lt;/code&gt; [1].&lt;/p&gt;

&lt;h2 id=&quot;looking-then-moving&quot;&gt;‘Looking’, then ‘moving’&lt;/h2&gt;

&lt;p&gt;As Hutchinson, Hager and Corke reflect in their work:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Vision is a useful robotic sensor since it mimics the human sense of vision and allows for non-contact measurement of the environment. […] Typically visual sensing and manipulation are combined in a open-loop fashion, ‘looking’ then ‘moving’.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So the goal of &lt;code&gt;Visual servo control&lt;/code&gt; is to control the movement and location of the robot using visual techniques (detection and recognition of objects in an image). To get an idea how it works, we must have clear some fundamental concepts in this field&lt;/p&gt;

&lt;h3 id=&quot;kinematics-of-a-robot&quot;&gt;Kinematics of a robot&lt;/h3&gt;

&lt;p&gt;We need to know what a kinematic chain is, what reference system and transformation coordinate are anda what algorithm is executed inside the robot kinematic. These concepts were explained in the second post of this collection. If you have doubts, consult it.&lt;/p&gt;

&lt;p&gt;If we link the kinematic chains concept with visual techniques (ie, now, in addition to the chain formed by the motors of the robotic arm, we have a camera in the chain looking one of the chain ends), we have two types of systems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Endpoint open-loop (EOL): Systems which only observed the target object. These systems don’t need to look at his end effector so normally the camera is on the end effector (hand-eye).&lt;/li&gt;
  &lt;li&gt;Endpoint closed-loop (ECL): Systems which observed the target object and the end effector of the arm.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The visual inverse kinematics that we implemented in Robocomp uses this last configuration because is independent of hand-eye calibration errors (precisely, the clearances errors and inaccuracies that bother us in the inverse kinematics), although often requires solution of a more demanding vision problem, because we need to track the end effector.&lt;/p&gt;

&lt;h3 id=&quot;camera-projection-models&quot;&gt;Camera Projection Models&lt;/h3&gt;

&lt;p&gt;We need to understand the geometric aspects of the imaging process if we want to understand how the information provided by the vision system is used to control the movement of the robot. The first thing to consider is that an image taken by a camera is always in 2D, so we’re losing spatial information (the depth of the scene).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://masters.donntu.org/2012/etf/nikitin/library/article10.files/image10.01.png&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To resolve this issue we have several options:
1. We can use multiple cameras that capture the studio space from different positions.
2. We can obtain multiple views with a single camera.
3. We can have previously stored the geometric relationship between certain characteristics of the target or the elements in the studio space.&lt;/p&gt;

&lt;p&gt;In any case, we must keep in mind certain things common to all cameras. For example the system of axes: the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; axes form the basis of the image plane and the &lt;code&gt;Z&lt;/code&gt; axis is perpendicular to the image plane, along the optical axis of the camera. The origin is located on the &lt;code&gt;Z&lt;/code&gt; axis at a distance &lt;code&gt;λ&lt;/code&gt; of the image plane. That distance &lt;code&gt;λ&lt;/code&gt; is what we call focal length.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.hitl.washington.edu/artoolkit/documentation/images/ch03-17.gif&quot; alt=&quot;ALt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can map the position and the orientation of the end effector in space calculating the projective geometry of the camera. But this method, complicated in itself, increases their difficulty because we need &lt;code&gt;recognize&lt;/code&gt; the end effector in the picture, in addition to deriving the speed from the changes observed in each frame that the camera capture. For these reasons, in our visual inverse kinematic component, we use the algorithm proposed by Edwin Olson, &lt;code&gt;Apriltags&lt;/code&gt; [2] a visual fiducial system that uses a 2D barcode style &lt;code&gt;tag&lt;/code&gt; (binary, black and white synthetic brands), allowing full 6 DOF localization of features from a single image. Thus, if we put a apriltag in the end effector, we can get its position and orientation in a very simple way.&lt;/p&gt;

&lt;h2 id=&quot;visualbik-component&quot;&gt;visualBIK component&lt;/h2&gt;

&lt;p&gt;Having already some clear concepts, let us study how the component developed in this project, &lt;code&gt;visualBIK&lt;/code&gt;, works.&lt;/p&gt;

&lt;p&gt;Our component implements a simple state machine where waits the reception of a target position (a vector with traslations and rotations: [tx, ty, tz,    rx, ry, rz]) through its interface. When a target is received, the visualBIK send it to the inverse kinematics component like a &lt;code&gt;POSE6D&lt;/code&gt; target, and waits for him to finish running the target and placing the arm. As the end effector will be a little out of the target position (due to inaccuracies), the visualBIK will be prepared to correct this error:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It calculates the visual pose of the end effector (through apriltags, visualBIK receives the position of the end effector mark that the camera head sees).&lt;/li&gt;
  &lt;li&gt;After, it compute the error vector between the visual pose and the target pose.&lt;/li&gt;
  &lt;li&gt;With this error vector, visualBIK corrects the target pose and sends the new position to the inverse kinematics component.&lt;/li&gt;
  &lt;li&gt;This process is repeated until the error achieved in translation and rotation is less than a predetermined threshold.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this way we can correct the errors introduced by the inaccuracies of the joints.&lt;/p&gt;

&lt;p&gt;This component (like component inverse kinematics) is still in the testing phase and is more than likely suffer some changes that improve its operation.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;[1] Hutchinson, S., Hager, G., Corke, P. &lt;code&gt;A Tutorial on Visual Servo Control&lt;/code&gt;, IEEE Trans. Robot. Automat., 12(5):651–670, Oct. 1996. Download in http://www-cvr.ai.uiuc.edu/~seth/ResPages/pdfs/HutHagCor96.pdf&lt;/p&gt;

&lt;p&gt;[2] OLson, E. &lt;code&gt;AprilTag: A robust and flexible visual fiducial system&lt;/code&gt;, Robotics and Automation (ICRA), 2011 IEEE International Conference on, 3400-3407&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>Debian Packaging &lt;p&gt;#2&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/06/15/nithin7.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/15/nithin7.html</guid>
        <description>&lt;h2 id=&quot;source-packages-and-ppa&quot;&gt;Source packages and ppa&lt;/h2&gt;
&lt;p&gt;A Personal Package Archive (PPA) s a special software repository for uploading source packages to be built and published as an APT repository by Launchpad.So basically if a software has a ppa then users can just add the pa to their sources and they will be able to install the software package and will also get updates automatically. As alreasy mentioned we can only upload source packages into a ppa, by definition &lt;em&gt;Source packages provide you with all of the necessary files to compile or otherwise, build the desired piece of software.&lt;/em&gt; now the next question is how can we create source packages. I have explained it in tutorial &lt;a href=&quot;http://robocomp.github.io/website/2015/05/23/nithin1.html&quot;&gt;Debian packaging&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;implementation-in-robocomp&quot;&gt;Implementation in Robocomp&lt;/h2&gt;
&lt;p&gt;For creating source package for robocomp i wrote a cmake module &lt;em&gt;source_package&lt;/em&gt;.The module will basically copy the source in to another directory (currently &lt;em&gt;Debian&lt;/em&gt; in build folder) and will create the source tar.Then it will create all debian/ files dynamically. The script will be executed when the target &lt;code&gt;spackage&lt;/code&gt; is made.&lt;/p&gt;

&lt;p&gt;After creating the source packages one trouble i faces was in setting up (registering) the PGP keys. Once you have created a launchpad account you should sign the Ubuntu Code of Conduct.Then you can upload the package using &lt;code&gt;dput&lt;/code&gt; utility.&lt;/p&gt;

&lt;h3 id=&quot;nb&quot;&gt;NB&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;You cant upload a package with same name into same ppa again, launchpad will reject it. so you need to change the package name and hence the version, every time you upload. This is automatically taken care off by the script.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If there are any changes to the source, then you should upload the whole source into the ppa. Well, any changes to fies in &lt;em&gt;debian&lt;/em&gt; folder is not considered a source change. In Robocomp its implemented in such a way that if there is any changes to the source, then you have to change the Robocomp version.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Right now we are not generating the changelog automatically. But that is a feature we could add, generating changelog from the git commit messages.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>&lt;i&gt;GSoC,&lt;/i&gt; Symbolic planning techniques for recognizing objects domestic &lt;p&gt;#2&lt;/p&gt;</title>
        <link>http://robocomp.github.io/website/2015/06/15/mercedes2.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/15/mercedes2.html</guid>
        <description>&lt;p&gt;&lt;strong&gt;What is inverse kinematics?&lt;/strong&gt; : In this second post, although it may seem begin the house from the roof, let’s talk about how a robot moves its arms and hands in order to manipulate daily objects.&lt;/p&gt;

&lt;p&gt;The ultimate goal of this work is make the robot to be able to recognize certain daily objects in a house (for example a mug), and to manipulate these objects with its effectors (hands). To do this, one of the things we need to implement is the inverse kinematics of the robot. Although this is the last step, we start by inverse kinematics to be easier and more intuitive than object recognition (besides that we have almost finalized the cinematic component in Robocomp).&lt;/p&gt;

&lt;h3 id=&quot;what-does-the-inverse-kinematics&quot;&gt;What does the inverse kinematics?&lt;/h3&gt;

&lt;p&gt;A recurring problem in robotics is to give to robots a certain autonomy in terms of movement. Focusing on a practical and realistic example, as is the trajectory of a robotic arm from an initial position to a target point, the question is how does the robot move its arm from the starting pose to the final pose? or what values take its engines arm to reach the final position? This is the typical problem of inverse kinematics, which is responsible for calculating the angular values of a kinematic chain composed engines (joints) of the arm to reach a target position.&lt;/p&gt;

&lt;p&gt;But before we get down to work, we need to review a few concepts.&lt;/p&gt;

&lt;h3 id=&quot;previous-concepts&quot;&gt;Previous concepts&lt;/h3&gt;

&lt;h4 id=&quot;kinematic-chains&quot;&gt;kinematic chains&lt;/h4&gt;

&lt;p&gt;The first concept that we should be clear is the &lt;code&gt;kinematic chain&lt;/code&gt;. The kinematic chain is a set of elements that produce motion, deforming the chain to adapt it to movement. Kinematic chains are composed of two elements:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;joints&lt;/code&gt;: joints or motors that produce the movement. Each joint gives a degree of freedom.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;links&lt;/code&gt;: rigid segments that connect the joints together.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An example of kinematic chain in robotic is the arm of the robot, that is composed by all the motors that the robot has and the segments that connect this motors in order to create the arm form.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.sitenordeste.com/mecanica/images/cadena_cinematica.JPG&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;reference-systems-and-transformation-coordinate&quot;&gt;Reference systems and Transformation coordinate.&lt;/h4&gt;

&lt;p&gt;One of the problems of robot manipulators is to know where their structural elements are arranged in the space in which they move. We therefore need a referral system that puts or position the elements of the robot in the workspace. So, a &lt;code&gt;reference system&lt;/code&gt; is a set of agreements or conventions used by an observer to measure positions, rotations and other physical parameters of the system being studied. In our case, the arm of the robot is into the three-dimensional workspace (R³, with the axis X, Y and Z), where each components (for example, each joint) has one traslation (tx, ty, tz) and one rotation (rx, ry, rz). Therefore, the position of each component is given by a vector of six elements: &lt;code&gt;P=[tx, ty, tz,   rx, ry, rz]&lt;/code&gt; (the first three translational and three rotational recent). Normally, we represent the poses by homogeneous trasnformation matrices, which are of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    | R  T |
P = | 0  1 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;R&lt;/code&gt; is the rotation matrix and &lt;code&gt;T&lt;/code&gt; the traslation coordenates.&lt;/p&gt;

&lt;p&gt;One of the kinematic problems is that each motor (which can be moved and/or rotated with respect to the previous motor of the chain) has his own reference system, so if we want to calculate the position of a particular point or joint, we will have to make a number of changes (&lt;code&gt;transformations&lt;/code&gt;) to move from one reference system to another. For example, if we have the newt arm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X_1--------------X_2--------X_3-----O
       M_1&amp;gt;2          M_2&amp;gt;3    M_3&amp;gt;O
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;X_n&lt;/code&gt; represents the position of the joints, &lt;code&gt;-&lt;/code&gt; is the link that connects the joints, &lt;code&gt;o&lt;/code&gt; is the end effector of the arm and &lt;code&gt;M_n&amp;gt;m&lt;/code&gt; are the transformation matrices to change the reference system n to the system m, and we want to calculate the position of the end effector in the reference system of the joint X_1, we have to calculate this equation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Po_inX_1 = M_2&amp;gt;1 * M_3&amp;gt;2 * M_o&amp;gt;3 * Po_inO.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;problems-to-solve-the-inverse-kinematics&quot;&gt;Problems to solve the inverse kinematics.&lt;/h3&gt;

&lt;p&gt;If the forward kinematics is responsible for calculating the position of the end effector in a kinematic chain, given some angular values for the joints, the inverse kinematics is just the opposite: it is responsible for calculating the angle values of the joints so the end effector reaches a position. This last problem is much more difficult to solve. So difficult that we are forced to use generic mathematical methods that try to approach an optimal solution iteratively within a reasonable time. We have opted for an iterative method known as the &lt;code&gt;Levenberg-Marquardt&lt;/code&gt; or &lt;code&gt;damped least squares&lt;/code&gt; algorithm. This method is used for solving nonlinear least squares problems where a solution to decrease an error function is sought.&lt;/p&gt;

&lt;h3 id=&quot;inverse-kinematics-in-robocomp&quot;&gt;Inverse kinematics in Robocomp&lt;/h3&gt;

&lt;p&gt;As a result of the TFG, &lt;code&gt;Inverse kinematics in Social Robots&lt;/code&gt; [1], since 2014 Robocomp has a component [2] that is responsible for calculating the inverse kinematics of the social robot Ursus [3], developed by Robolab. This component has undergone a big evolution, since it was created last year to now, and is more than likely to continue evolving to achieve inverse kinematics each finer and in less time.&lt;/p&gt;

&lt;p&gt;Originally, this component receives three types of targets:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;POSE6D: It is the typical target with translations and rotations in the X, Y and Z axis. The end effector has to be positioned at coordinates (tx, ty, tz) of the target and align their rotation axes with the target, specified in (rx, ry, rz).&lt;/li&gt;
  &lt;li&gt;ADVANCEAXIS: its goal is to move the end effector of the robot along a vector. This is useful for improving the outcome of the above problem, for example, imagine that the hand has been a bit away from a mug. With this feature we can calculate the error vector between the end effector and the mug, and move the effector along the space to place it in an optimal position, near the mug.&lt;/li&gt;
  &lt;li&gt;ALIGNAXIS: Its goal is that the end effector is pointing to target without moving to it but rotated as the target. It may be useful in certain cases where we are more interested in oriented the end effector with the same rotation of the target.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To solve these various inverse kinematic problems, the component uses as main base the &lt;code&gt;Levenberg-Marquardt&lt;/code&gt; algorithm proposed in the article &lt;code&gt;SBA: A Software Package for Generic Sparse Bundle Adjustment&lt;/code&gt; by Lourakis and Argyros:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input: A vector functon f: R^m → R^n with n≥m, a measurement vector x ∈ R^n and an initial parameters estimate p_0 ∈ R^m.
Output: A vector p+ ∈ R^m minimizing ||x-f(p)||^2.
Algorithm:
    k:=0;                 v:=2;                     p:=p0;
    A:=transposed(J)·J;   error:=x-f(p);            g:=transposed(J)·error;
    stop:=(||g||∞ ≤ ε1);  μ:=t*max_i=1,...,m (Aii)
    
    while(!stop) and (k&amp;lt;k_max)
         k:=k+1;
         repeat
               SOLVE (A+μ·I)·δ_p=g;
               if(||δ_p||≤ ε2·(||p||+ε2))
                    stop:=true;
               else
                    p_new:=p+δ_p
                    ρ:=(||error||^2-||x-f(p_new)||^2)/(transposed(δ_p)·(μ·δ_p+g));
                    if ρ&amp;gt;0
                        stop:=(||error||-||x-f(p_new)||&amp;lt;ε4·||error||);
                        p:=p_new;
                        A:=transposed(J)·J;    error:=x-f(p);    g:=transposed(J)·error;
                        stop:=(stop) or (||g||∞ ≤ ε1);
                        μ:=μ*max(1/3, 1-(2·ρ-1)^3);
                        v:=2;
                    else
                        μ:=μ*v;
                        v:=2*v;
                    endif
               endif
         until(ρ&amp;gt;0) or (stop)
         stop:=(||error||≤ ε3);
    endwhile
    p+:=p;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;A&lt;/code&gt; is the hessian matrix, &lt;code&gt;J&lt;/code&gt; is the jacobian matrix, &lt;code&gt;g&lt;/code&gt; is the gradient descent, &lt;code&gt;δ_p&lt;/code&gt; is the increments, &lt;code&gt;ρ&lt;/code&gt; is the ratio of profit that tells us if we are approaching a minimum or not, &lt;code&gt;μ&lt;/code&gt; is the damping factor, and &lt;code&gt;t&lt;/code&gt; and &lt;code&gt;ε1, ε2, ε3, ε4&lt;/code&gt; are different thresholds. But the IK component of Robocomp adds several concepts to the original L-M algorithm, in order to complete the proper operation of the component:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Weight matrix: that controls the relevance between the translations (in meters) and rotations (in radians) of the target. So, where &lt;code&gt;g&lt;/code&gt; was calculated as &lt;code&gt;transposed(J)·error&lt;/code&gt;, now &lt;code&gt;g&lt;/code&gt; is &lt;code&gt;transposed(J)·(W·error)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Motors lock: when a motor reachs its minimun or maximun limit, we modified the jacobian matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The new version of the inverse kinematics component simplifies the code of the old version and adds some more functionality:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Executes more than once a target. The inverse kinematic result is not the same if the start point of the effector is the robot’s home or a point B near tho the goal point.&lt;/li&gt;
  &lt;li&gt;Executes the traslations without the motors of the wrisht (only for Ursus). This makes possible to move the arm with stiff wrist, and then we can rotate easely the wrist when the end effectos is near the target.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another improvement being studied is to include a small planner responsible for planning the trajectories of the robot arm, in order to facilitate the work of the IK component and reduce its execution time. However, one of the problems that the inverse kinematics can not solve by itself is the problem of gaps and imperfections of the robot. These gaps and inaccuracies make the robot move its arm toward the target position improperly, so that the robot “thinks” that the end effector has reached the target but in reality has fallen far short of the target pose.&lt;/p&gt;

&lt;p&gt;In order to solve this last problem, we need visual feedback to correct the errors and mistakes introduced for the gaps and inaccuracies in the kinematic chain. The visualBIK component, developed during this project, is responsible for solve this visual feedback and correct the inverse kinematic, but we’ll talk about it in the next post.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;[1] Master Thesis, Universidad de Extremadura, Escuela Politécnica de Cáceres. Mercedes Paoletti Ávila. &lt;code&gt;Cinemática Inversa en Robots Sociales&lt;/code&gt;. Directed by Pablo Bustos and Luis Vicente Calderita. July 2014. Download in https://robolab.unex.es/index.php?option=com_remository&amp;amp;Itemid=53&amp;amp;func=startdown&amp;amp;id=143&lt;/p&gt;

&lt;p&gt;[2] inverse kinematics component repository: https://github.com/robocomp/robocomp-ursus/tree/master/components/inversekinematics&lt;/p&gt;

&lt;p&gt;[3] C. Suárez Mejías, C. Echevarría, P. Núñez, L. Manso, P. Bustos, S. Leal and C. Parra. &lt;code&gt;Ursus: A Robotic Assistant for Training of Patients with Motor Impairments&lt;/code&gt;. Book, Converging Clinical and Engineering Research on Neurorehabilitation, Springer series on BioSystems and BioRobotics, Editors, J.L Pons, D. Torricelli and Marta Pajaro. Springer, ISBN 978-3-642-34545-6, pages 249-254. January 2012. Download in https://robolab.unex.es/index.php?option=com_remository&amp;amp;Itemid=53&amp;amp;func=startdown&amp;amp;id=128&lt;/p&gt;

&lt;p&gt;[4] Lourakis, M. I., Argyros, A. (2009). &lt;code&gt;SBA: A Software Package for Generic Sparse Bundle Adjustment&lt;/code&gt;. Article of ACM Transactions on Mathematical Software, volume 36, issue 1, pages 1-30. Download in http://doi.acm.org/10.1145/1486527&lt;/p&gt;

</description>
        <pubDate>Mon, 15 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
      <item>
        <title>Debian Packaging</title>
        <link>http://robocomp.github.io/website/2015/06/12/nithin8.html</link>
        <guid isPermaLink="true">http://robocomp.github.io/website/2015/06/12/nithin8.html</guid>
        <description>&lt;h2 id=&quot;what-is-a-binary-package&quot;&gt;What is a binary package?&lt;/h2&gt;
&lt;p&gt;A binary package in a is an application package which contains (pre-built) executables, as opposed to source code. Basically a binary package is an archive which contains executables some other info like rules on how to install them, dependencies etc. debian binary package is also a type of binary package. You can use a package manger to install these packages.I have explained I have explained it in tutorial &lt;a href=&quot;http://robocomp.github.io/website/2015/05/23/nithin1.html&quot;&gt;Debian packaging&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;implementation-in-robocomp&quot;&gt;Implementation in Robocomp&lt;/h2&gt;
&lt;p&gt;For binary packages i was left with two options. whether i could use the same cmake script i used for creating source packages or i could use the &lt;code&gt;CPACK&lt;/code&gt; packaging tool. Finally i decided to go with CPACK because - 1)less code , that means less messing up 2)its an well known tool so is expected to perform better than a script i am writing. CPACK has so many configuration options so i made a seperate cmake file &lt;code&gt;package_details.cmake&lt;/code&gt; for configuring cpack so that its easier for users to change any configuration. CPACK will add a target &lt;code&gt;package&lt;/code&gt; for generating binary package. so you could run &lt;code&gt;make package&lt;/code&gt; for generating the package.&lt;/p&gt;

&lt;h2 id=&quot;source-packages-and-ppa&quot;&gt;Source packages and ppa&lt;/h2&gt;
&lt;p&gt;A Personal Package Archive (PPA) s a special software repository for uploading source packages to be built and published as an APT repository by Launchpad.So basically if a software has a ppa then users can just add the pa to their sources and they will be able to install the software package and will also get updates automatically. As alreasy mentioned we can only upload source packages into a ppa, by definition &lt;em&gt;Source packages provide you with all of the necessary files to compile or otherwise, build the desired piece of software.&lt;/em&gt; now the next question is how can we create source packages. I have explained it in tutorial &lt;a href=&quot;http://robocomp.github.io/website/2015/05/23/nithin1.html&quot;&gt;Debian packaging&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;implementation-in-robocomp-1&quot;&gt;Implementation in Robocomp&lt;/h2&gt;
&lt;p&gt;For creating source package for robocomp i wrote a cmake module &lt;em&gt;source_package&lt;/em&gt;.The module will basically copy the source in to another directory (currently &lt;em&gt;Debian&lt;/em&gt; in build folder) and will create the source tar.Then it will create all debian/ files dynamically. The script will be executed when the target &lt;code&gt;spackage&lt;/code&gt; is made.&lt;/p&gt;

&lt;p&gt;After creating the source packages one trouble i faces was in setting up (registering) the PGP keys. Once you have created a launchpad account you should sign the Ubuntu Code of Conduct.Then you can upload the package using &lt;code&gt;dput&lt;/code&gt; utility.&lt;/p&gt;

&lt;h3 id=&quot;nb&quot;&gt;NB&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Unfortunately CPACK has a bug in it, its not changing the control files permission correctly which throws a warning during installation. so i have create a bash script which will fix the control file permissions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You cant upload a package with same name into same ppa again, launchpad will reject it. so you need to change the package name and hence the version, every time you upload. This is automatically taken care off by the script.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If there are any changes to the source, then you should upload the whole source into the ppa. Well, any changes to fies in &lt;em&gt;debian&lt;/em&gt; folder is not considered a source change. In Robocomp its implemented in such a way that if there is any changes to the source, then you have to change the Robocomp version.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Right now we are not generating the changelog automatically. But that is a feature we could add, generating changelog from the git commit messages.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Jun 2015 00:00:00 +0530</pubDate>
      </item>
    
  </channel>
</rss>
